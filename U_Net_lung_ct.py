# -*- coding: utf-8 -*-
"""U-Net.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rysW9fylJxqZFUBQEoOTLYbCh28z3Do6
"""

import kagglehub

# Download dataset via kaggle api
path = kagglehub.dataset_download("kmader/finding-lungs-in-ct-data")

print("Path to dataset files:", path)

import albumentations as A
from albumentations.pytorch import ToTensorV2
from torch.utils.data import Dataset, Subset, DataLoader
import os
import cv2
from sklearn.model_selection import train_test_split
import torch
import torch.nn as nn

# Define dataset class for segmentation data
'''
  Inputs:
    images_dir
    masks_dir
    transform
  Outputs:
    image, mask
'''
class SegDataset(Dataset):
  def __init__(self, images_dir, masks_dir, transform=None):
    self.images_dir = images_dir
    self.masks_dir = masks_dir
    self.images = sorted(os.listdir(self.images_dir))
    self.transform = transform

  def __len__(self):
    return len(self.images)

  def __getitem__(self, index):
    image_path = os.path.join(self.images_dir, self.images[index])
    mask_path = os.path.join(self.masks_dir, self.images[index])
    image = cv2.imread(image_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)

    if self.transform:
      transformed = self.transform(image=image, mask=mask)
      image = transformed['image']
      mask = transformed['mask']
      '''
        Mask values are 0 and 255 binary, to normalize it we should divide by
        255 and convert to float for the loss function. Also should add a
        dimention of 1 in the begining for the loss function
      '''
      mask = mask / 255.0
      mask = mask.unsqueeze(0)
      # mask = mask.float()

    return image, mask

'''
  data_loader function
  Inputs:
    images_dir
    masks_dir
    test_size
    batch_size
  Outputs:
    train_loader, val_loader, test_loader
'''
def data_loader(images_dir, masks_dir, test_size=0.1, batch_size=32):
  # Use data augmentation on train dataset
  train_transformer = A.Compose([
      A.Resize(128, 128),
      A.HorizontalFlip(p=0.5),
      A.VerticalFlip(p=0.2),
      A.RandomRotate90(p=0.5),
      A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.3),
      # albumenation normalizer only normalizes the images not the masks
      A.Normalize(mean=(0, 0, 0), std=(255, 255, 255), max_pixel_value=255.0),
      ToTensorV2()
  ])

  val_test_transformer = A.Compose([
      A.Resize(128, 128),
      A.Normalize(mean=(0, 0, 0), std=(255, 255, 255), max_pixel_value=255.0),
      ToTensorV2()
  ])

  full_dataset = SegDataset(images_dir=images_dir, masks_dir=masks_dir)
  full_dataset_indices = list(range(len(full_dataset)))

  # Extract indices of train and test
  train_indices, test_indices = train_test_split(full_dataset_indices, test_size=test_size, random_state=42)

  test_dataset = Subset(SegDataset(images_dir=images_dir, masks_dir=masks_dir, transform=val_test_transformer), test_indices)
  test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

  # Extract indices of train and validation
  train_indices, val_indices = train_test_split(train_indices, test_size=test_size, random_state=42)
  train_dataset = Subset(SegDataset(images_dir=images_dir, masks_dir=masks_dir, transform=train_transformer), train_indices)
  val_dataset = Subset(SegDataset(images_dir=images_dir, masks_dir=masks_dir, transform=val_test_transformer), val_indices)
  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
  val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

  return train_loader, val_loader, test_loader

'''
  DoubleConv class to be used in U_Net architecture
  Inputs:
    in_channels, out_channels
'''
class DoubleConv(nn.Module):
  def __init__(self, in_channels, out_channels):
    super().__init__()
    self.conv = nn.Sequential(
        nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1),
        nn.ReLU(inplace=True),
        nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, padding=1),
        nn.ReLU(inplace=True)
    )

  def forward(self, x):
    return self.conv(x)


class UNet(nn.Module):
  def __init__(self, n_classes):
    super().__init__()
    self.encoder1 = DoubleConv(3, 64)
    self.pool1 = nn.MaxPool2d(kernel_size=2)
    self.encoder2 = DoubleConv(64, 128)
    self.pool2 = nn.MaxPool2d(kernel_size=2)
    self.bottleneck = DoubleConv(128, 256)
    self.up1 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=2, stride=2)
    self.decoder1 = DoubleConv(256, 128)
    self.up2 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=2, stride=2)
    self.decoder2 = DoubleConv(128, 64)
    self.out_conv = nn.Conv2d(in_channels=64, out_channels=n_classes, kernel_size=1)

  def forward(self, x):
    e1 = self.encoder1(x)
    e2 = self.pool1(e1)
    e2 = self.encoder2(e2)
    b = self.pool2(e2)
    b = self.bottleneck(b)
    d1 = self.up1(b)
    d1 = torch.cat([d1, e2], dim=1)
    d1 = self.decoder1(d1)
    d2 = self.up2(d1)
    d2 = torch.cat([d2, e1], dim=1)
    d2 = self.decoder2(d2)
    out = self.out_conv(d2)

    return out

device = "cuda" if torch.cuda.is_available() else "cpu"
model = UNet(n_classes=1).to(device)

num_epochs = 20

criterion = nn.BCEWithLogitsLoss()   # binary segmentation
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

train_loader, val_loader, test_loader = data_loader(images_dir=path+'/2d_images', masks_dir=path+'/2d_masks')

for epoch in range(num_epochs):
  # Turn on dropouts and randomization
  model.train()
  train_loss = 0.0
  for images, masks in train_loader:
    images, masks = images.to(device), masks.to(device)
    outputs = model(images) # Compute models output
    loss = criterion(outputs, masks) # Compute loss
    train_loss += loss.item()

    optimizer.zero_grad() # Clear gradients
    loss.backward() # Compute gradients
    optimizer.step() # Update model's parameters based on gradients
  train_loss /= len(train_loader)
  print(f'Epoch: {epoch}/{num_epochs}, Loss: {train_loss:.4f}')

  # Turn off gradient update
  with torch.no_grad():
    # Turn off dropouts and randomization
    model.eval()
    val_loss = 0.0
    for images, masks in val_loader:
      images, masks = images.to(device), masks.to(device)
      outputs = model(images)
      loss = criterion(outputs, masks)
      val_loss += loss.item()
    val_loss /= len(val_loader)
    print(f'Validation loss: {val_loss:.4f}')

